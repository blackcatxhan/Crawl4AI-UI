import os
import json
import asyncio
import gradio as gr
import time
from pydantic import BaseModel, Field
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy, JsonCssExtractionStrategy
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from crawl4ai.content_filter_strategy import PruningContentFilter
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy
from crawl4ai.deep_crawling.filters import FilterChain, URLPatternFilter
from dotenv import load_dotenv

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env (n·∫øu c√≥)
load_dotenv()

# Bi·∫øn to√†n c·ª•c cho crawler ƒë·ªÉ t√°i s·ª≠ d·ª•ng
crawler_instance = None
loop = None

class CrawlApp:
    def __init__(self):
        self.browser_config = BrowserConfig(headless=True)
        
    async def init_crawler(self):
        """Kh·ªüi t·∫°o v√† tr·∫£ v·ªÅ crawler n·∫øu ch∆∞a t·ªìn t·∫°i"""
        global crawler_instance
        if crawler_instance is None:
            crawler_instance = AsyncWebCrawler(config=self.browser_config)
            await crawler_instance.start()
        return crawler_instance
        
    async def crawl_url(self, url, cache_mode="BYPASS", content_threshold=0.1, extraction_mode="markdown"):
        """H√†m th·ª±c hi·ªán crawl theo URL v√† c·∫•u h√¨nh"""
        # T·∫°o b·ªô l·ªçc n·ªôi dung v√† tr√¨nh t·∫°o markdown
        content_filter = PruningContentFilter(threshold=float(content_threshold), threshold_type="fixed")
        md_generator = DefaultMarkdownGenerator(content_filter=content_filter)
        
        # C·∫•u h√¨nh CrawlerRunConfig c∆° b·∫£n
        run_config = CrawlerRunConfig(
            cache_mode=getattr(CacheMode, cache_mode),
            markdown_generator=md_generator
        )
        
        # Th√™m chi·∫øn l∆∞·ª£c tr√≠ch xu·∫•t n·∫øu c·∫ßn
        if extraction_mode == "css" and hasattr(self, 'css_schema') and self.css_schema:
            try:
                schema = json.loads(self.css_schema)
                run_config = run_config.clone(
                    extraction_strategy=JsonCssExtractionStrategy(schema)
                )
            except Exception as e:
                return f"L·ªói schema CSS: {str(e)}", None, "{}", None
        elif extraction_mode == "llm" and hasattr(self, 'llm_schema') and self.llm_schema:
            # Ch·ªâ s·ª≠ d·ª•ng LLM v·ªõi API key ho·∫∑c m√¥ h√¨nh c·ª•c b·ªô
            provider = self.llm_provider if hasattr(self, 'llm_provider') else "ollama/llama3.3"
            api_key = self.llm_api_key if hasattr(self, 'llm_api_key') else None
            
            if (provider.startswith("openai") and not api_key):
                return "C·∫ßn API key cho OpenAI", None, "{}", None
                
            try:
                schema = json.loads(self.llm_schema)
                instruction = self.llm_instruction if hasattr(self, 'llm_instruction') else "Tr√≠ch xu·∫•t d·ªØ li·ªáu theo schema"
                
                run_config = run_config.clone(
                    extraction_strategy=LLMExtractionStrategy(
                        llm_config=LLMConfig(provider=provider, api_token=api_key),
                        schema=schema,
                        extraction_type="schema",
                        instruction=instruction,
                        extra_args={"temperature": 0, "top_p": 0.9}
                    )
                )
            except Exception as e:
                return f"L·ªói schema LLM: {str(e)}", None, "{}", None
        
        # Th·ª±c hi·ªán crawl
        start_time = time.time()
        try:
            # L·∫•y instance crawler hi·ªán c√≥ ho·∫∑c t·∫°o m·ªõi
            crawler = await self.init_crawler()
            
            # Th·ª±c hi·ªán crawl
            result = await crawler.arun(url=url, config=run_config)
            duration = time.time() - start_time
            
            # Hi·ªÉn th·ªã th√¥ng tin v·ªÅ vi·ªác s·ª≠ d·ª•ng cache
            cache_info = "t·ª´ cache" if cache_mode == "ENABLED" and duration < 0.5 else "m·ªõi crawl"
            
            # Ki·ªÉm tra v√† ƒë·∫£m b·∫£o k·∫øt qu·∫£ ƒë√∫ng khi d√πng cache
            if hasattr(result, 'markdown'):
                # Chu·∫©n b·ªã k·∫øt qu·∫£ tr·∫£ v·ªÅ
                if hasattr(result.markdown, 'fit_markdown'):
                    markdown_content = result.markdown.fit_markdown
                elif hasattr(result.markdown, 'raw_markdown'):
                    markdown_content = result.markdown.raw_markdown
                else:
                    markdown_content = str(result.markdown)
                
                # ƒê·∫£m b·∫£o markdown_content kh√¥ng tr·ªëng
                if not markdown_content or markdown_content.strip() == "":
                    if cache_mode == "ENABLED":
                        # Th·ª≠ crawl l·∫°i v·ªõi BYPASS n·∫øu k·∫øt qu·∫£ t·ª´ cache tr·ªëng
                        bypass_config = run_config.clone(cache_mode=CacheMode.BYPASS)
                        result = await crawler.arun(url=url, config=bypass_config)
                        if hasattr(result.markdown, 'fit_markdown'):
                            markdown_content = result.markdown.fit_markdown
                        elif hasattr(result.markdown, 'raw_markdown'):
                            markdown_content = result.markdown.raw_markdown
                        else:
                            markdown_content = str(result.markdown)
                        cache_info = "cache tr·ªëng, ƒë√£ crawl l·∫°i"
            else:
                markdown_content = "Kh√¥ng c√≥ n·ªôi dung Markdown"
            
            # X·ª≠ l√Ω extracted_json (ƒë·∫£m b·∫£o kh√¥ng tr·∫£ v·ªÅ None khi d√πng JSON component)
            extracted_json = result.extracted_content if hasattr(result, 'extracted_content') and result.extracted_content else "{}"
            
            # ƒê·∫£m b·∫£o extracted_json l√† ƒë·ªëi t∆∞·ª£ng JSON th·ª±c s·ª±, kh√¥ng ph·∫£i chu·ªói
            try:
                if isinstance(extracted_json, str):
                    extracted_json = json.loads(extracted_json)
                # N·∫øu JSON r·ªóng, th√™m d·ªØ li·ªáu m·∫´u ƒë·ªÉ ki·ªÉm tra hi·ªÉn th·ªã
                if not extracted_json:
                    extracted_json = {"info": "Kh√¥ng c√≥ d·ªØ li·ªáu JSON", "url": url}
                print(f"JSON type: {type(extracted_json)}, content: {str(extracted_json)[:100]}")
            except Exception as json_error:
                print(f"L·ªói chuy·ªÉn ƒë·ªïi JSON: {str(json_error)}")
                # ƒê·∫£m b·∫£o lu√¥n tr·∫£ v·ªÅ object JSON h·ª£p l·ªá
                extracted_json = {"error": "D·ªØ li·ªáu JSON kh√¥ng h·ª£p l·ªá", "url": url}
            
            # Chuy·ªÉn ƒë·ªïi k·∫øt qu·∫£ Markdown sang text ƒë·ªÉ c√≥ th·ªÉ sao ch√©p
            text_content = markdown_content
            
            status = f"‚úÖ Crawl th√†nh c√¥ng ({cache_info})! Th·ªùi gian: {duration:.2f}s"
            return status, markdown_content, extracted_json, text_content
        except Exception as e:
            duration = time.time() - start_time
            return f"‚ùå L·ªói: {str(e)} (sau {duration:.2f}s)", None, "{}", None
    
    def set_css_schema(self, schema):
        """L∆∞u schema CSS"""
        self.css_schema = schema
        return "ƒê√£ l∆∞u schema CSS"
    
    def set_llm_config(self, provider, api_key, schema, instruction):
        """L∆∞u c·∫•u h√¨nh LLM"""
        self.llm_provider = provider
        self.llm_api_key = api_key
        self.llm_schema = schema
        self.llm_instruction = instruction
        return "ƒê√£ l∆∞u c·∫•u h√¨nh LLM"
    
    def crawl_url_sync(self, url, cache_mode, content_threshold, extraction_mode):
        """Wrapper ƒë·ªìng b·ªô cho crawl_url"""
        global loop
        if loop is None:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(
            self.crawl_url(url, cache_mode, content_threshold, extraction_mode)
        )
    
    async def crawl_deep_bfs(self, url, max_depth, max_pages, include_external, cache_mode="BYPASS", content_threshold=0.1):
        """H√†m th·ª±c hi·ªán deep crawl v·ªõi BFS strategy"""
        # T·∫°o b·ªô l·ªçc n·ªôi dung v√† tr√¨nh t·∫°o markdown
        content_filter = PruningContentFilter(threshold=float(content_threshold), threshold_type="fixed")
        md_generator = DefaultMarkdownGenerator(content_filter=content_filter)
        
        # T·∫°o BFS Deep Crawl Strategy
        bfs_strategy = BFSDeepCrawlStrategy(
            max_depth=int(max_depth),
            max_pages=int(max_pages),
            include_external=include_external
        )
        
        # C·∫•u h√¨nh CrawlerRunConfig
        run_config = CrawlerRunConfig(
            cache_mode=getattr(CacheMode, cache_mode),
            markdown_generator=md_generator,
            deep_crawl_strategy=bfs_strategy,
            stream=True
        )
        
        # Th·ª±c hi·ªán crawl
        start_time = time.time()
        results_data = []
        
        # L∆∞u tr·ªØ d·ªØ li·ªáu ƒë·∫ßy ƒë·ªß c·ªßa m·ªói trang
        all_results = []
        
        # Theo d√µi URL ƒë√£ x·ª≠ l√Ω ƒë·ªÉ tr√°nh tr√πng l·∫∑p
        processed_urls = set()
        
        try:
            # L·∫•y instance crawler hi·ªán c√≥ ho·∫∑c t·∫°o m·ªõi
            crawler = await self.init_crawler()
            
            # Th·ª±c hi·ªán deep crawl v·ªõi streaming mode
            async for result in await crawler.arun(url=url, config=run_config):
                # Ki·ªÉm tra URL c√≥ b·ªã tr√πng l·∫∑p kh√¥ng
                current_url = result.url
                if current_url in processed_urls:
                    continue  # B·ªè qua URL ƒë√£ x·ª≠ l√Ω
                
                # Th√™m URL v√†o danh s√°ch ƒë√£ x·ª≠ l√Ω
                processed_urls.add(current_url)
                
                # L·∫•y th√¥ng tin markdown
                if hasattr(result, 'markdown'):
                    if hasattr(result.markdown, 'fit_markdown'):
                        markdown_content = result.markdown.fit_markdown
                    elif hasattr(result.markdown, 'raw_markdown'):
                        markdown_content = result.markdown.raw_markdown
                    else:
                        markdown_content = str(result.markdown)
                else:
                    markdown_content = "Kh√¥ng c√≥ n·ªôi dung Markdown"
                
                # L·∫•y th√¥ng tin metadata
                depth = result.metadata.get('depth', 0) if hasattr(result, 'metadata') else 0
                
                # X·ª≠ l√Ω extracted_json n·∫øu c√≥
                json_content = result.extracted_content if hasattr(result, 'extracted_content') and result.extracted_content else "{}"
                
                # ƒê·∫£m b·∫£o json_content l√† ƒë·ªëi t∆∞·ª£ng JSON th·ª±c s·ª±, kh√¥ng ph·∫£i chu·ªói
                try:
                    if isinstance(json_content, str):
                        json_content = json.loads(json_content)
                    # N·∫øu JSON r·ªóng, th√™m d·ªØ li·ªáu m·∫´u c√≥ c·∫•u tr√∫c
                    if not json_content:
                        json_content = {
                            "url": current_url,
                            "depth": depth,
                            "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                            "status": "success"
                        }
                    print(f"Deep JSON type: {type(json_content)}, content: {str(json_content)[:100]}")
                except Exception as json_error:
                    print(f"L·ªói chuy·ªÉn ƒë·ªïi JSON deep crawl: {str(json_error)}")
                    # ƒê·∫£m b·∫£o lu√¥n tr·∫£ v·ªÅ object JSON h·ª£p l·ªá
                    json_content = {
                        "error": "D·ªØ li·ªáu JSON kh√¥ng h·ª£p l·ªá", 
                        "url": current_url,
                        "depth": depth
                    }
                
                # L∆∞u th√¥ng tin ƒë·∫ßy ƒë·ªß c·ªßa trang
                all_results.append({
                    "url": current_url,
                    "depth": depth,
                    "markdown": markdown_content,
                    "text": markdown_content,
                    "json": json_content
                })
                
                # Th√™m k·∫øt qu·∫£ v√†o danh s√°ch hi·ªÉn th·ªã
                results_data.append([
                    current_url,
                    depth,
                    markdown_content[:200] + "..." if len(markdown_content) > 200 else markdown_content
                ])
            
            duration = time.time() - start_time
            status = f"‚úÖ Deep crawl ho√†n t·∫•t! ƒê√£ crawl {len(results_data)}/{max_pages} trang ƒë·ªôc nh·∫•t. Th·ªùi gian: {duration:.2f}s"
            
            # M·∫∑c ƒë·ªãnh hi·ªÉn th·ªã n·ªôi dung c·ªßa trang ƒë·∫ßu ti√™n (n·∫øu c√≥)
            first_result = all_results[0] if all_results else {"markdown": "", "text": "", "json": {"info": "Kh√¥ng c√≥ d·ªØ li·ªáu"}}
            
            # Ki·ªÉm tra lo·∫°i d·ªØ li·ªáu JSON tr∆∞·ªõc khi tr·∫£ v·ªÅ
            print(f"First result JSON type: {type(first_result['json'])}")
            
            return status, results_data, first_result["markdown"], first_result["text"], first_result["json"], all_results
        except Exception as e:
            duration = time.time() - start_time
            return f"‚ùå L·ªói: {str(e)} (sau {duration:.2f}s)", [], "", "", {"error": str(e)}, []

    def crawl_deep_bfs_sync(self, url, max_depth, max_pages, include_external, cache_mode, content_threshold):
        """Wrapper ƒë·ªìng b·ªô cho crawl_deep_bfs"""
        global loop
        if loop is None:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(
            self.crawl_deep_bfs(url, max_depth, max_pages, include_external, cache_mode, content_threshold)
        )
    
    def get_selected_result(self, evt: gr.SelectData, results):
        """L·∫•y th√¥ng tin chi ti·∫øt c·ªßa k·∫øt qu·∫£ ƒë∆∞·ª£c ch·ªçn t·ª´ b·∫£ng"""
        if not results or evt.index[0] >= len(results):
            return "", "", {}
        
        selected = results[evt.index[0]]
        return selected["markdown"], selected["text"], selected["json"]

# Kh·ªüi t·∫°o ·ª©ng d·ª•ng
app = CrawlApp()

# H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng CSS
CSS_GUIDE = """
### H∆∞·ªõng d·∫´n c·∫•u h√¨nh CSS Selector

CSS selector gi√∫p b·∫°n tr√≠ch xu·∫•t d·ªØ li·ªáu c√≥ c·∫•u tr√∫c t·ª´ trang web d·ª±a tr√™n c√°c th·∫ª HTML v√† class.

**ƒê·ªãnh d·∫°ng schema c∆° b·∫£n:**
```json
{
    "name": "T√™n nh√≥m d·ªØ li·ªáu",
    "baseSelector": "CSS selector g·ªëc ƒë·ªÉ ch·ªçn nhi·ªÅu ph·∫ßn t·ª≠",
    "fields": [
        {
            "name": "t√™n_tr∆∞·ªùng",
            "selector": "selector con",
            "type": "text|attribute|html|innerHtml", 
            "attribute": "t√™n_thu·ªôc_t√≠nh" (ch·ªâ c·∫ßn khi type l√† attribute)
        }
    ]
}
```

**V√≠ d·ª• th·ª±c t·∫ø - Tr√≠ch xu·∫•t s·∫£n ph·∫©m t·ª´ trang th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠:**
```json
{
    "name": "Products",
    "baseSelector": ".product-item",
    "fields": [
        {"name": "title", "selector": "h3.product-title", "type": "text"},
        {"name": "price", "selector": ".price-current", "type": "text"},
        {"name": "image", "selector": "img.product-image", "type": "attribute", "attribute": "src"},
        {"name": "url", "selector": "a.product-link", "type": "attribute", "attribute": "href"}
    ]
}
```

**M·∫πo s·ª≠ d·ª•ng:**
1. S·ª≠ d·ª•ng DevTools trong tr√¨nh duy·ªát (F12) ƒë·ªÉ x√°c ƒë·ªãnh CSS selectors
2. Selector `.classname` ch·ªçn ph·∫ßn t·ª≠ theo class
3. Selector `#id` ch·ªçn ph·∫ßn t·ª≠ theo ID
4. Selector `element` ch·ªçn t·∫•t c·∫£ th·∫ª HTML ƒë√≥ (vd: `a`, `div`, `img`)
5. C√°c types: 
   - `text`: L·∫•y n·ªôi dung vƒÉn b·∫£n
   - `attribute`: L·∫•y gi√° tr·ªã thu·ªôc t√≠nh (href, src, alt, v.v)
   - `html`: L·∫•y HTML ƒë·∫ßy ƒë·ªß bao g·ªìm th·∫ª
   - `innerHtml`: L·∫•y HTML b√™n trong th·∫ª

Sau khi l∆∞u schema, ch·ªçn ch·∫ø ƒë·ªô tr√≠ch xu·∫•t "css" ·ªü tab Crawl c∆° b·∫£n ƒë·ªÉ s·ª≠ d·ª•ng.
"""

# H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng LLM
LLM_GUIDE = """
### H∆∞·ªõng d·∫´n c·∫•u h√¨nh LLM (Language Model)

LLM gi√∫p b·∫°n tr√≠ch xu·∫•t d·ªØ li·ªáu c√≥ c·∫•u tr√∫c t·ª´ trang web b·∫±ng c√°ch s·ª≠ d·ª•ng m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn.

**C√°c th√†nh ph·∫ßn c·∫•u h√¨nh:**

1. **LLM Provider**: Ch·ªçn nh√† cung c·∫•p v√† m√¥ h√¨nh
   - **Ollama** (m√¥ h√¨nh c·ª•c b·ªô, mi·ªÖn ph√≠): c·∫ßn c√†i ƒë·∫∑t Ollama tr√™n m√°y t√≠nh
   - **OpenAI** (API c√≥ ph√≠): y√™u c·∫ßu API key

2. **API Key**: C·∫ßn thi·∫øt cho OpenAI. Kh√¥ng b·∫Øt bu·ªôc cho Ollama.

3. **Schema LLM**: C·∫•u tr√∫c d·ªØ li·ªáu b·∫°n mu·ªën tr√≠ch xu·∫•t, ·ªü d·∫°ng JSON v·ªõi ki·ªÉu d·ªØ li·ªáu.

4. **H∆∞·ªõng d·∫´n tr√≠ch xu·∫•t**: Ch·ªâ d·∫´n c·ª• th·ªÉ cho LLM v·ªÅ c√°ch tr√≠ch xu·∫•t d·ªØ li·ªáu.

**V√≠ d·ª• schema cho trang s·∫£n ph·∫©m:**
```json
{
    "title": "string",
    "price": "string",
    "description": "string", 
    "rating": "string",
    "specifications": {
        "brand": "string",
        "model": "string",
        "size": "string",
        "weight": "string"
    }
}
```

**V√≠ d·ª• h∆∞·ªõng d·∫´n tr√≠ch xu·∫•t:**
```
T·ª´ trang s·∫£n ph·∫©m, h√£y tr√≠ch xu·∫•t:
1. Ti√™u ƒë·ªÅ ch√≠nh c·ªßa s·∫£n ph·∫©m
2. Gi√° hi·ªán t·∫°i (b·ªè qua gi√° g·ªëc)
3. M√¥ t·∫£ s·∫£n ph·∫©m (ƒëo·∫°n vƒÉn ƒë·∫ßu ti√™n)
4. ƒê√°nh gi√° (s·ªë sao ho·∫∑c ƒëi·ªÉm)
5. Th√¥ng s·ªë k·ªπ thu·∫≠t bao g·ªìm th∆∞∆°ng hi·ªáu, m·∫´u, k√≠ch th∆∞·ªõc, v√† tr·ªçng l∆∞·ª£ng n·∫øu c√≥
```

**L∆∞u √Ω quan tr·ªçng:**
- Khi s·ª≠ d·ª•ng OpenAI, ƒë·∫£m b·∫£o API key c·ªßa b·∫°n c√≤n h·∫°n d√πng
- M√¥ h√¨nh m·ªõi nh·∫•t th∆∞·ªùng cho k·∫øt qu·∫£ t·ªët nh·∫•t (GPT-4.1, GPT-4o)
- Ollama y√™u c·∫ßu c√†i ƒë·∫∑t v√† ch·∫°y tr√™n m√°y t√≠nh c·ª•c b·ªô
- Tr√≠ch xu·∫•t b·∫±ng LLM ch·∫≠m h∆°n CSS nh∆∞ng th√¥ng minh h∆°n, ph√π h·ª£p v·ªõi d·ªØ li·ªáu ph·ª©c t·∫°p

Sau khi l∆∞u c·∫•u h√¨nh, ch·ªçn ch·∫ø ƒë·ªô tr√≠ch xu·∫•t "llm" ·ªü tab Crawl c∆° b·∫£n ƒë·ªÉ s·ª≠ d·ª•ng.
"""

# X√¢y d·ª±ng giao di·ªán Gradio
with gr.Blocks(title="Crawl4AI UI", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# üï∏Ô∏è Crawl4AI - Giao di·ªán ng∆∞·ªùi d√πng")
    
    with gr.Tabs():
        with gr.TabItem("Crawl c∆° b·∫£n"):
            with gr.Row():
                with gr.Column():
                    url_input = gr.Textbox(label="URL", placeholder="https://example.com")
                    with gr.Row():
                        cache_mode = gr.Dropdown(
                            choices=["BYPASS", "ENABLED"], 
                            value="BYPASS", 
                            label="Ch·∫ø ƒë·ªô cache"
                        )
                        content_threshold = gr.Slider(
                            minimum=0.1, maximum=1.0, value=0.1, step=0.1,
                            label="Ng∆∞·ª°ng n·ªôi dung"
                        )
                    extraction_mode = gr.Radio(
                        choices=["markdown", "css", "llm"], 
                        value="markdown",
                        label="Ch·∫ø ƒë·ªô tr√≠ch xu·∫•t"
                    )
                    crawl_btn = gr.Button("B·∫Øt ƒë·∫ßu Crawl", variant="primary")
                
                with gr.Column():
                    status_output = gr.Textbox(label="Tr·∫°ng th√°i")
                    with gr.Tabs():
                        with gr.TabItem("Markdown"):
                            markdown_output = gr.Markdown(label="K·∫øt qu·∫£ Markdown")
                        with gr.TabItem("Text"):
                            text_output = gr.Textbox(
                                label="K·∫øt qu·∫£ VƒÉn b·∫£n",
                                lines=20,
                                max_lines=50,
                                show_copy_button=True
                            )
                        with gr.TabItem("JSON"):
                            json_output = gr.JSON(label="K·∫øt qu·∫£ JSON")
        
        with gr.TabItem("Deep Crawl (BFS)"):
            with gr.Row():
                with gr.Column():
                    deep_url_input = gr.Textbox(label="URL", placeholder="https://example.com")
                    with gr.Row():
                        max_depth = gr.Slider(
                            minimum=1, maximum=3, value=1, step=1,
                            label="ƒê·ªô s√¢u t·ªëi ƒëa"
                        )
                        max_pages = gr.Slider(
                            minimum=5, maximum=100, value=20, step=5,
                            label="S·ªë trang t·ªëi ƒëa"
                        )
                    with gr.Row():
                        include_external = gr.Checkbox(
                            value=False,
                            label="Bao g·ªìm link ngo√†i"
                        )
                        deep_cache_mode = gr.Dropdown(
                            choices=["BYPASS", "ENABLED"], 
                            value="BYPASS", 
                            label="Ch·∫ø ƒë·ªô cache"
                        )
                        deep_content_threshold = gr.Slider(
                            minimum=0.1, maximum=1.0, value=0.1, step=0.1,
                            label="Ng∆∞·ª°ng n·ªôi dung"
                        )
                    deep_crawl_btn = gr.Button("B·∫Øt ƒë·∫ßu Deep Crawl", variant="primary")
                
                with gr.Column():
                    deep_status_output = gr.Textbox(label="Tr·∫°ng th√°i")
                    with gr.Tabs():
                        with gr.TabItem("K·∫øt qu·∫£"):
                            deep_results = gr.Dataframe(
                                headers=["URL", "ƒê·ªô s√¢u", "N·ªôi dung t√≥m t·∫Øt"],
                                label="K·∫øt qu·∫£ Deep Crawl (Click v√†o m·ªôt d√≤ng ƒë·ªÉ xem chi ti·∫øt)",
                                wrap=True,
                                interactive=False
                            )
                        with gr.TabItem("Markdown"):
                            deep_markdown_output = gr.Markdown(label="N·ªôi dung Markdown")
                        with gr.TabItem("Text"):
                            deep_text_output = gr.Textbox(
                                label="N·ªôi dung vƒÉn b·∫£n",
                                lines=20,
                                max_lines=50,
                                show_copy_button=True
                            )
                        with gr.TabItem("JSON"):
                            deep_json_output = gr.JSON(label="K·∫øt qu·∫£ JSON", visible=True)
                    
                    # Bi·∫øn ·∫©n ƒë·ªÉ l∆∞u tr·ªØ t·∫•t c·∫£ k·∫øt qu·∫£
                    all_deep_results = gr.State([])
        
        with gr.TabItem("C·∫•u h√¨nh CSS"):
            
            with gr.Accordion("Nh·∫≠p Schema CSS", open=True):
                css_schema = gr.Textbox(
                    label="Schema CSS (JSON)", 
                    placeholder='''{
    "name": "Example Items",
    "baseSelector": "div.item",
    "fields": [
        {"name": "title", "selector": "h2", "type": "text"},
        {"name": "link", "selector": "a", "type": "attribute", "attribute": "href"}
    ]
}''',
                    lines=10
                )
                save_css_btn = gr.Button("L∆∞u schema CSS")
                css_status = gr.Textbox(label="Tr·∫°ng th√°i")

            gr.Markdown(CSS_GUIDE)
        
        with gr.TabItem("C·∫•u h√¨nh LLM"):
            
            with gr.Accordion("C·∫•u h√¨nh LLM", open=True):
                with gr.Row():
                    llm_provider = gr.Dropdown(
                        choices=["ollama/llama3.3", "openai/gpt-4.1", "openai/gpt-4.1-mini", "openai/gpt-4.1-nano", "openai/gpt-4o", "openai/gpt-4o-mini"], 
                        value="ollama/llama3.3",
                        label="LLM Provider"
                    )
                    llm_api_key = gr.Textbox(
                        label="API Key (c·∫ßn thi·∫øt cho OpenAI)", 
                        placeholder="sk-...",
                        type="password"
                    )
                
                llm_schema = gr.Textbox(
                    label="Schema LLM (JSON)",
                    placeholder='''{
    "title": "string",
    "description": "string",
    "price": "string",
    "rating": "string"
}''',
                    lines=8
                )
                
                llm_instruction = gr.Textbox(
                    label="H∆∞·ªõng d·∫´n tr√≠ch xu·∫•t",
                    placeholder="Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ, m√¥ t·∫£, gi√° v√† ƒë√°nh gi√° t·ª´ trang s·∫£n ph·∫©m n√†y.",
                    lines=3
                )
                
                save_llm_btn = gr.Button("L∆∞u c·∫•u h√¨nh LLM")
                llm_status = gr.Textbox(label="Tr·∫°ng th√°i")

            gr.Markdown(LLM_GUIDE)
    
    # ƒêƒÉng k√Ω c√°c h√†m x·ª≠ l√Ω s·ª± ki·ªán
    crawl_btn.click(
        app.crawl_url_sync,
        inputs=[url_input, cache_mode, content_threshold, extraction_mode],
        outputs=[status_output, markdown_output, json_output, text_output]
    )
    
    deep_crawl_btn.click(
        app.crawl_deep_bfs_sync,
        inputs=[deep_url_input, max_depth, max_pages, include_external, deep_cache_mode, deep_content_threshold],
        outputs=[deep_status_output, deep_results, deep_markdown_output, deep_text_output, deep_json_output, all_deep_results]
    )
    
    # S·ª± ki·ªán khi ng∆∞·ªùi d√πng b·∫•m v√†o m·ªôt d√≤ng trong b·∫£ng k·∫øt qu·∫£
    deep_results.select(
        app.get_selected_result,
        inputs=[all_deep_results],
        outputs=[deep_markdown_output, deep_text_output, deep_json_output]
    )
    
    save_css_btn.click(
        app.set_css_schema,
        inputs=[css_schema],
        outputs=[css_status]
    )
    
    save_llm_btn.click(
        app.set_llm_config,
        inputs=[llm_provider, llm_api_key, llm_schema, llm_instruction],
        outputs=[llm_status]
    )

# Kh·ªüi ch·∫°y giao di·ªán
if __name__ == "__main__":
    # Kh·ªüi t·∫°o v√≤ng l·∫∑p s·ª± ki·ªán
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    # K·∫øt th√∫c crawler khi ·ª©ng d·ª•ng ƒë√≥ng
    try:
        demo.launch()
    finally:
        if crawler_instance is not None:
            loop.run_until_complete(crawler_instance.stop())
        loop.close() 